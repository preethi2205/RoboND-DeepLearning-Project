[//]: # (Image References)

[imageQuadSimulator]: ./Pictures/imageQuadSimulator.PNG
[imagePatch]: ./Pictures/imagePatch.png
[imageCovNetExample]: ./Pictures/imageCovNetExample.PNG
[imageFCNArchitecture]: ./Pictures/imageFCNArchitecture.PNG
[imageMyFCNArch]: ./Pictures/imageMyFCN.PNG
[imageTrainingCurves]: ./Pictures/imageTrainingCurve.PNG
[imageTargetIdentified]: ./Pictures/imageTargetIdentified.png
[imageTargetIdentified2]: ./Pictures/imageTargetIdentified2.png
[imageTargetIdentified3]: ./Pictures/imageTargetIdentified3.png
[imageFalseTargetIgnored]: ./Pictures/imageFalseTargetIgnored.png
[imageFalseTargetIgnored2]: ./Pictures/imageFalseTargetIgnored2.png
[imageFarTargetIdentified]: ./Pictures/imageFarTargetIdentified.png
[imageFarTargetMissed]: ./Pictures/imageFarTargetMissed.png
[imageTargetFollowingScore]: ./Pictures/imageTargetFollowingScore.PNG
[imageNoTargetScore]: ./Pictures/imageNoTargetScore.PNG
[imageFarTargetScore]: ./Pictures/imageFarTargetScore.PNG
[imageFinalScore]: ./Pictures/imageFinalScore.PNG

## Project: Deep Learning for robotics
### The why? 
This project is meant to give an introduction to the application of neural networks in the field of robotic perception. Neural networks are becoming a popular end-to-end solution in an increasing number of fields. The idea of training artificial intelligence to perform complex cognitive tasks is exciting. When done properly, it has the ability to supplement or even surpass classical techniques in the respective fields. The advantage comes from the fact that a properly trained neural network is capable of performing complex tasks with ease. 

### The what?
The robot involved in this project is a quadcopter, fitted with a front facing camera. Given the camera output (a video stream), our aim is to train a neural network that is able to identify a specific person. This task is complicated, as there are a lot of variables involved. For example, the scene could be crowded. The perspective from which the person is viewed is different each time. If the person is far away, there are very few pixels describing the person, which makes it harder for the neural network to identify. The quadcopter uses the neural network's output to follow her around the park. The task of understanding a scene and figuring out where an object lies in the scene is called scene understanding. Developing a neural network that is able to assign meaning to each pixel in the image is called semantic segmentation. The following sections describe how we develop our neural network to do semantic segmentation.

### The how?
In this project, we train a "fully connected network" (FCN) to perform scene understanding via semantic segmentation. The data used to train the neural network is obtained by using a simulator provided by Udacity. The simulator has a training and an autonomous mode. The training mode is used to manually control the quadcopter (keyboard/mouse) to obtain the training data. Once the training is complete, the trained model can be used to run the quadcopter in the autonomous mode. Here's a screenshot from the simulator:
![alt text][imageQuadSimulator]

The images are generated by steering the quadcopter through various scenarios where it follows a given person. Some examples of the different scenarios are - predefined path following behind the target, scenes with people who are not the target, a crowded scene with the target, different angles from which the target can be seen. Once the training data is available, we architect a fully connected network and train it. The output of the training is a "model", which can then be fed back into the simulator to evaluate the solution.

 The following sections will give an overview of FCNs, the network architecture, the hyper parameters used for training, and will finally provide some results. 

### Overview of neural networks
In layman terms, a neural network is a system that can be trained to identify complex boundaries between clusters of similar points. As the complexity of these boundaries increases, the number of layers in the neural network also increases correspondingly, thus leading to "deep neural networks". 

#### Convolutions and Convolutional Neural Networks (CovNets/ CNNs)
Consider an image as an input to a neural network, and the output being true or false (based on whether an image contains a particular object). The neural network is made of multiple layers. Each layer in the neural network learns something about the image pixels (color, texture, specific features). For this task, we do not care about spatial information. That is, we do not care about the location of the object in the image. Consider the object of interest to be a "patch". Each layer in the NN tries to learn something new about this patch. A filter which is the same size of the patch is run over the image according a requested stride. This new information is added as "depth" to the layer. Intuitively, each progressive layer gets smaller in height and width, and bigger in depth (smaller patches, greater information). This operation is known as "convolution". Here's a pictorial representation of a convolution operation (Source: Udacity):
![alt text][imagePatch]

The layers in the neural network keep shrinking until enough information about the patch is learnt. Here's an example of a typical CovNet architecture:
![alt text][imageCovNetExample]

#### Fully Connected Networks
So far, we've only been concerned with whether an object is present in a given image. However, for scene understanding, we need to identify the object and find its location. In other words, the "patch" that we described in the previous section needs to be scaled up to fit in the original image. A neural network used for scene understanding is made of two parts:

1. The encoder, or the CovNet that identifies the object: The encoder module can be derived easily using transfer learning. It is typically derived from a pre-trained neural network available for image classification tasks (VGG, ImageNet, for example). In our project we use two techniques to improve the performance of the encoder layers:

	1. Separable convolution: A technique where convolution is first applied to each image layer (depth) and the results are then combined to produce the output. This technique reduces the number of convolutions needed, and cleverly uses matrix multiplications to speed up the process. [This link is a good source that talks about the different types of convolutions, including separable convolution.](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)
	
	2. Batch normalization: The inputs to any neural network are normalized to avoid any scaling issues in the input features. To improve performance, batch normalization is applied at the input of every encoder layer, rather than just at the input of the network.

2. The decoder, or the part that scales up the object to the original image size. The decoder is responsible for scaling up the image. Two common techniques can be used to scale up images:
	1. Transpose convolution layers, where each pixel is multiplied by a filter (patch) to upscale the image.  
	2. Bilinear upsampling, where each pixel is derived by averaging the closest neighbors.
Bilinear upsampling is not as accurate as transpose convolutions, although much lesser in computation cost, as it does not involve any training.

In an FCN architecture, the encoder and decoder are connected by a 1x1 convolution layer (instead of a fully connected layer). This 1x1 convolution layer is a filter (patch) of size 1, with a stride of 1. That is, it operates on each pixel, rather than operating on a patch of pixels. There are a few advantages of using a 1x1 convolution layer instead of a fully connected layer:
1. Using a fully connected layer requires that the output be flattened to a 2D tensor, thus resulting in loss of spatial information. A 1x1 convolution layer is used to preserve  spatial information about the object of interest. 
2. The 1x1 convolution layer also helps in adding depth (more parameters to train, more features to learn) to the network.
3. The 1x1 convolution layer in not computationally expensive, as the math simplifies to a set of matrix multiplications. 

The final piece relevant to FCNs are skip layers. In the encoder, when the image is scaled down, some of the spatial information is lost at each level. Skip layers are direct connections between an encoder and a decoder layer. It attempts to add back the spatial information that was lost.

Here's an example of a typical FCN architecture:
![alt text][imageFCNArchitecture]

### Network Architecture
In this project, we use three encoder layers of depth 32, 64 and 128 respectively. Each encoder layer has a convolution layer with stride 2. The final encoder layer is connected to a 1x1 convolution layer, with depth 128. The decoders contain a bilinear upsampling layer with an upsample factor of 2, followed by a convolution layer. We use three decoders of filter size 128, 64 and 32. Each encoder layer is connected to the corresponding decoding layer through a skip connection. Refer to the figure below for an illustration of the architecture. As an experiment, additional encoder layers of filter size 256 and 512 were added intermediately, but did not provide much increase in training accuracy. Thus, the final architecture used a maximum depth of 128.
![alt text][imageMyFCNArch]

### Hyper parameters
Hyper parameters are tunable optimizer parameters, that play a major role in training the network. The hyper parameters that were tweaked are listed below, along with an explanation for the chosen parameter values.

1. **Learning rate: 0.001**
The learning rate dictates the rate of change of the weights. If the learning rate is too high, the weights may fluctuate too much, thus never converging. If the learning rate is too low, the weights will not change much, and might not be able to reach the desired value. I started with a learning rate of 0.0001, and achieved a training loss of about 0.025. On experimenting, I found that a higher learning rate helped in faster learning and lower training loss. A learning rate of 0.01 was too high, leading to too many fluctuations in the training and validation loss. I settled on a value of 0.001 for the final learning rate.
2. **Batch size: 100**
Batch size is the number of images processed at once. Since I used the Udacity's GPU workspace to train my network, I was able to use a large batch size of 100.
3. **Number of epochs: 250**
This is the number of times the training is run. I found that after about 225 epochs, there is not much improvement in the training loss. Although 250 epochs took up a lot of training time, it resulted in a low training loss and good prediction accuracy. Please see the section about future enhancements for more thoughts on this.
4. **Number of steps per epoch: 75**
This is the number of times that the images are processed per epoch. Higher number of steps are used for increased accuracy, without using additional data. I found that with about 200 epochs, 75 steps per epoch yielded in very good accuracy.
5. **Validation steps: 50**
I used 50 validation steps per epoch to validate the data. Using 50 validation steps seemed to give an accurate representation of the training accuracy, without taking up too much of computation time.
6. **Workers: 2**
2 workers were used to do the training. 

### Training Results
The training curves (training and validation loss over epochs) for the final model is given below:

![alt text][imageTrainingCurves]

The prediction is applied to three different scenarios to obtain an initial evaluation. Sample predictions for each scenario are given below. The first column is the image input to the network. The second column is the ground truth and the final column is our network's predicted output.

**Image where target is available to follow**

![alt text][imageTargetIdentified]
![alt text][imageTargetIdentified2]
![alt text][imageTargetIdentified3]

**Image with people who are not the targets**

![alt text][imageFalseTargetIgnored]
![alt text][imageFalseTargetIgnored2]

**Crowded and distant scenes**

![alt text][imageFarTargetIdentified]
![alt text][imageFarTargetMissed]

A metric called the IOU (Intersection Over Union) is used to measure a score for the semantic segmentation operation. An example of what the IOU means for the target identification:

1. Intersection Set is the number of pixels that belong to the target in the ground truth and are truly identified as a target by the network. 
2. Union set is the number of pixels that either belong to the ground truth target, or are classified as the target by the network.

Here's the IOU we obtained for the target identification scenario:

![alt text][imageTargetFollowingScore]

The IOU for the false target rejection is:

![alt text][imageNoTargetScore]

Finally, the IOU for the crowded and distant scene identification is:

![alt text][imageFarTargetScore]

The average IOU we obtained 0.43:

![alt text][imageFinalScore]

### Potential problems and further suggestions

As we can see, there is a lot of room for improvement in the IOU score. Here are few of the areas that can be improved:

1. It can be seen that the network has problems identifying the target in crowded scenes that are sensed from a distance. Data augmentation can be used to improve training under such conditions. 
2. Additional data can also be added by flipping the images, in order to improve the robustness of the network.
3. 250 epochs were used to train the network with 75 steps per epoch. It might be time saving to use far fewer epochs, with a larger number of steps per epoch to increase the training accuracy.
4. More depth can be added to the architecture by doubling the 1x1 convolution layer to a depth of 256.
5. A limitation of this model is that this particular FCN has been trained on human images. It is not generalizable to other scene recognition tasks which involve identification of animals or cars, for example. 
6. An optimization algorithm can be run to pick optimal values for the hyper parameters.

### Source code for the project

1. [Model setup and training](./code/model_training.ipynb)
2. [Model weights](./data/weights/model_weights.h5)
